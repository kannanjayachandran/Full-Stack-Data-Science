# ‚öôÔ∏è Results Summary

| Version                                          | Concurrency Type                          | Time (seconds) | Speedup vs Sync  | Notes                                                                                               |
| ------------------------------------------------ | ----------------------------------------- | -------------- | ---------------- | --------------------------------------------------------------------------------------------------- |
| **Synchronous**                                  | Single-threaded (sequential blocking I/O) | **7.94 s**     | 1√ó               | Baseline ‚Äî every request waits for the previous one to finish                                       |
| **Threading (`concurrent.futures`, 20 workers)** | OS threads (I/O-bound concurrency)        | **0.43 s**     | **~18√ó faster**  | Excellent scaling ‚Äî threads efficiently overlap I/O waits                                           |
| **Asyncio (`aiohttp`)**                          | Single-threaded event loop (async I/O)    | **1.23 s**     | **~6.4√ó faster** | Still very fast, but slower than threads due to per-request coroutine overhead and connection setup |

---

### üß© Why These Numbers Make Sense

#### 1. **Threading Wins Here**

* `requests` uses blocking I/O under the hood.
* `ThreadPoolExecutor` + 20 workers ‚Üí true parallelism for I/O waits (network latency hidden).
* Each thread runs `session.get()` independently, reusing its own TCP session via `thread_local`.

#### 2. **Asyncio Slightly Slower**

* `aiohttp` connections are async, but each call still needs to open its own socket (unless you reuse sessions carefully).
* Python‚Äôs async model avoids thread-switching but adds coroutine scheduling overhead.
* It excels when the concurrency count is **hundreds to thousands** ‚Äî at 80 requests, threading has less overhead.

#### 3. **Synchronous**

* No concurrency, total latency is basically the sum of all network response times.

---

### ‚öñÔ∏è **When Each Is Best**

| Use Case                   | Best Model                          | Reason                                                          |
| -------------------------- | ----------------------------------- | --------------------------------------------------------------- |
| Network requests (10‚Äì200)  | ‚úÖ **Threading**                     | Simpler, high performance, minimal setup                        |
| High-volume HTTP (1000+)   | ‚úÖ **Asyncio**                       | Scales without threads, lower memory footprint                  |
| CPU-heavy tasks            | ‚ùå Neither (use **multiprocessing**) | Threads and asyncio don‚Äôt parallelize CPU-bound code due to GIL |
| Simple scripts / debugging | ‚úÖ **Sync**                          | Straightforward and predictable                                 |

---

### üß† Optional Experiment

If you want to **see asyncio outperform threading**, increase workload:

```python
DO_NOT_ABUSE_THE_SERVER = 400  # or use a mock local server
```

Then you‚Äôll likely observe:

* Threaded version slows down (context-switch overhead)
* Asyncio stays more stable (event loop scales better)
